{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwPEy+QR3qo2SBnDmfcGEy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucasabbruzzini/Portfolio/blob/main/DL_GPW_2_Step_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "try:\n",
        "    import keras_tuner as kt\n",
        "except:\n",
        "    !pip install keras_tuner\n",
        "    import keras_tuner as kt\n",
        "\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "SEED = 42\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "tf.config.experimental.enable_op_determinism()\n",
        "\n",
        "sns.set(style=\"whitegrid\")"
      ],
      "metadata": {
        "id": "QzDtO2LNI5Jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.a) Preprocess data\n",
        "\n",
        "Building upon the foundational development in Step 2, we have engineered two classes: Asset and Portfolio. These classes represent a sophisticated object-oriented approach to managing financial data, reflecting a deliberate design choice rooted in software engineering principles and financial modeling requirements. The decision to implement classes in this context stems from their capacity to encapsulate data and behavior, providing a robust framework for handling complex financial datasets. This approach offers several advanced benefits:\n",
        "\n",
        "Data Encapsulation: The Asset class consolidates all relevant financial metrics (e.g., prices, returns, volatility) into a single object, eliminating the need for disparate variable management across multiple assets. This enhances data integrity and reduces cognitive overhead.\n",
        "Code Reusability: The class design facilitates the instantiation of multiple Asset objects (e.g., for SPY, TLT) with uniform behavior, leveraging polymorphism and inheritance potential to avoid redundant code implementation.\n",
        "Extensibility: The object-oriented paradigm allows for seamless integration of advanced financial metrics—such as dividend yields or risk-adjusted performance measures like the Sharpe ratio—through method augmentation, ensuring scalability without architectural overhaul.\n",
        "Asset Aggregation: The Portfolio class centralizes the management of diverse Asset objects, functioning as a structured container that simplifies portfolio-level operations and enhances data coherence.\n",
        "Analytical Facilitation: By consolidating returns into a single DataFrame, it supports advanced econometric analyses, such as correlation studies or input preparation for machine learning models (e.g., LSTM networks for return prediction), optimizing computational efficiency.\n",
        "Flexibility and Scalability: The class structure permits the incorporation of portfolio and risk management techniques to be explored in the next courses.\n",
        "The Asset class serves as a modular entity, encapsulating the attributes and behaviors of individual financial instruments, such as equities or exchange-traded funds (e.g., SPY, a proxy for the S&P 500). This class is architected to manage intricate financial computations, including the retrieval of historical price series, computation of daily logarithmic returns, and estimation of volatility metrics. It initializes with parameters such as ticker symbol, temporal range, data frequency (defaulting to daily), and a rolling window parameter for statistical analysis, subsequently populating instance variables with processed data.\n",
        "\n",
        "The Portfolio class acts as a higher-level aggregator, synthesizing multiple Asset instances into a cohesive investment portfolio. It is initialized with a portfolio identifier, ownership details, and a list of ticker symbols, subsequently populating its asset collection. This class provides analytical capabilities, including the aggregation of asset returns into a unified DataFrame and the generation of a comprehensive portfolio valuation report."
      ],
      "metadata": {
        "id": "laRyrryIJJaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Asset:\n",
        "    def __init__(self, ticker, start_date, end_date, frequency='1d', window=20):\n",
        "        # Here set up the basics for this asset:\n",
        "        self.ticker = ticker\n",
        "        self.start_date = pd.Timestamp(start_date)\n",
        "        self.end_date = pd.Timestamp(end_date)\n",
        "        self.frequency = frequency\n",
        "        self.window = window\n",
        "        # Now we'll grab the price data, compute returns, and figure out the volatility.\n",
        "        self.prices = self._download_data()\n",
        "        self.returns = self.calculate_returns()\n",
        "        self.volatility = self.calculate_volatility()\n",
        "\n",
        "    def _download_data(self):\n",
        "        import time\n",
        "        retries = 3  # We'll give it 3 tries in case something goes wrong with the download. Note that there was difficults during the creation of this code due to too many requests\n",
        "        for attempt in range(retries):\n",
        "            try:\n",
        "                print(f\"Trying to grab price data for {self.ticker} from {self.start_date} to {self.end_date}...\")\n",
        "                time.sleep(2)  # A little pause to avoid overwhelming Yahoo Finance's servers and get 429 response\n",
        "                data = yf.download(self.ticker, start=self.start_date, end=self.end_date, interval=self.frequency)\n",
        "                if data.empty:\n",
        "                    print(f\"Uh-oh, no data came back for {self.ticker}.\")\n",
        "                    raise ValueError(f\"No price data for {self.ticker}. Check ticker or connection.\")\n",
        "                print(f\"Got the data for {self.ticker} successfully!\")\n",
        "                # We'll use the adjusted close price if available; otherwise, the regular close.\n",
        "                return data['Adj Close'].squeeze() if 'Adj Close' in data else data['Close'].squeeze()\n",
        "            except Exception as e:\n",
        "                if attempt < retries - 1:\n",
        "                    print(f\"Download failed for {self.ticker} (attempt {attempt + 1}/{retries}): {e}. Let's try again...\")\n",
        "                    time.sleep(2 ** attempt)  # Wait a bit longer each time before retrying.\n",
        "                else:\n",
        "                    print(f\"That's it, we couldn't get the data for {self.ticker}: {e}\")\n",
        "                    raise ValueError(f\"No price data for {self.ticker} after {retries} attempts. Check ticker or connection.\")\n",
        "\n",
        "    def calculate_returns(self):\n",
        "        # This calculates the percentage change in prices day-to-day to get the returns, filling any missing values with 0.\n",
        "        return self.prices.pct_change().fillna(0)\n",
        "\n",
        "    def calculate_volatility(self, annualize=True):\n",
        "        # Here we calculate the volatility, which tells us how much the returns fluctuate over a 20-day window (or whatever window we set).\n",
        "        volatility = self.returns.rolling(window=self.window).std()\n",
        "        # annualize it\n",
        "        return volatility * np.sqrt(252) if annualize else volatility\n",
        "\n",
        "class Portfolio:\n",
        "    def __init__(self, name, owner, tickers, frequency='1d', window=20):\n",
        "        # We're setting up a portfolio with a name, owner, and a list of tickers (like SPY, TLT, etc.).\n",
        "        self.name = name\n",
        "        self.owner = owner\n",
        "        self.frequency = frequency  # How often we want the data (daily by default).\n",
        "        self.window = window  # Window for calculations like volatility.\n",
        "        self.assets = []  # This will hold our Asset objects.\n",
        "        self.tickers = tickers\n",
        "        # Let's create an Asset object for each ticker, covering the date range from 2008 to 2022.\n",
        "        self.assets = [Asset(ticker, \"2008-01-01\", \"2022-12-30\", self.frequency, self.window) for ticker in self.tickers]\n",
        "\n",
        "    def get_returns_df(self):\n",
        "        # This method gathers the returns for all assets in the portfolio and puts them into a single DataFrame.\n",
        "        if not self.assets:\n",
        "            raise ValueError(\"Oops, there are no assets in the portfolio yet. Try creating some first!\")\n",
        "        return pd.DataFrame({asset.ticker: asset.returns for asset in self.assets}).fillna(0)\n",
        "\n",
        "    def display_portfolio(self):\n",
        "        # This prints out a summary of the portfolio, showing the total value and the latest price for each asset.\n",
        "        if not self.assets:\n",
        "            raise ValueError(\"Hold on, we don't have any assets to display. Create some assets first!\")\n",
        "        prices_df = pd.DataFrame({asset.ticker: asset.prices for asset in self.assets}).dropna(how='all')\n",
        "        print(f\"\\nPortfolio: {self.name}\")\n",
        "        print(f\"Owner: {self.owner}\")\n",
        "        print(f\"Date Range: 2008-01-01 to 2022-12-30\")\n",
        "        print(f\"Total Value (latest prices): ${prices_df.iloc[-1].sum():,.2f}\")\n",
        "        print(\"Assets:\")\n",
        "        for ticker in prices_df.columns:\n",
        "            price = prices_df[ticker].iloc[-1] if not prices_df[ticker].isna().all() else 0\n",
        "            print(f\"  - {ticker}: ${price:,.2f}\")"
      ],
      "metadata": {
        "id": "Yx1LOYk9JMen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets choose the tickers and initialize our Portfolio\n",
        "tickers = [\"SPY\", \"TLT\", \"SHY\", \"GLD\", \"DBO\"]\n",
        "portfolio = Portfolio(\"Growth Fund\", \"John Doe\", tickers)\n",
        "portfolio.display_portfolio()\n",
        "df_ret = portfolio.get_returns_df()\n",
        "print(\"Portfolio creation completed.\")"
      ],
      "metadata": {
        "id": "AIhzaYuYJOUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3.b) Build multi-output Model:Train and Test multi-output Model:\n",
        "\n",
        "The MultiOutputLSTM class is a custom implementation for multi-output time series forecasting, designed to predict future returns for multiple stock tickers simultaneously using a Long Short-Term Memory (LSTM) neural network as studied in module 5. It leverages historical stock return data, constructs technical indicators (e.g., moving averages, volatility), and employs a deep learning architecture to capture temporal dependencies. The class integrates data preprocessing, model building, hyperparameter tuning, training, and performance evaluation, making it a comprehensive tool for financial time series analysis.\n",
        "\n",
        "Key Components\n",
        "\n",
        "Data Preparation:\n",
        "\n",
        "Features: The gen_predictors method generates inputs like 10-day and 50-day moving averages and 20-day volatility, alongside a 25-day future return as the target (Ret25). These are standard financial indicators capturing trend and risk.\n",
        "Splitting: Data is split chronologically—pre-2018 for training/validation (70%/30%) and 2018–2022 for testing—reflecting a realistic out-of-sample evaluation.\n",
        "Scaling: Inputs are standardized (StandardScaler), and outputs are normalized per ticker (MinMaxScaler) to stabilize LSTM training. Lagging: The create_lags method creates sequences of window_size (default 30) timesteps, enabling the LSTM to learn from historical patterns.\n",
        "Model Architecture:\n",
        "\n",
        "The LSTM stack consists of three layers (two with return_sequences=True, one without), followed by dense layers (50, 20, and output nodes equal to the number of tickers). This depth captures complex temporal relationships, while dropout mitigates overfitting. Loss is mean squared error (MSE), optimized with Adam, suitable for regression tasks like return prediction.\n",
        "Training and Evaluation:\n",
        "\n",
        "Early stopping (patience=10) and learning rate scheduling (ReduceLROnPlateau) enhance training efficiency.\n",
        "\n",
        "The analyze_performance method visualizes predictions versus actuals, aiding interpretability.\n",
        "\n",
        "Keras Tuner Integration\n",
        "\n",
        "The tune_hyperparameters method uses keras_tuner’s BayesianOptimization to optimize three hyperparameters:\n",
        "\n",
        "units_lstm: Integer range [50, 300] (step 50). Controls LSTM layer capacity—larger values increase expressivity but risk overfitting.\n",
        "n_dropout: Float range [0.0, 0.5] (step 0.1). Dropout rate balances regularization; higher values reduce overfitting but may underfit if excessive.\n",
        "hp_lr: Float range [1e-5, 1e-2] (log sampling). Learning rate governs convergence speed—log sampling explores a wide range efficiently.\n",
        "Parameter Choices and Non-Optimal Settings\n",
        "\n",
        "Default Parameters: units_lstm=150, n_dropout=0.1, hp_lr=1e-3, epochs=20, and max_trials=5 (in tuning). These are reasonable starting points but not necessarily optimal:\n",
        "units_lstm=150 balances capacity and complexity but may not suit all datasets.\n",
        "n_dropout=0.1 is conservative; higher values might improve generalization.\n",
        "hp_lr=1e-3 is a common default, but the optimal rate varies by problem.\n",
        "Epochs and Max Trials:\n",
        "\n",
        "epochs=20 limits training duration, potentially halting before full convergence, especially for noisy financial data requiring longer learning.\n",
        "max_trials=5 restricts hyperparameter exploration, risking suboptimal configurations.\n",
        "These conservative values reflect computational limitations (e.g., CPU/GPU availability, memory). Ideally, epochs could be 50–100, and max_trials 20–50, allowing deeper training and broader tuning. Modern hardware (e.g., TPUs) or cloud resources could support this, but without such access, we trade optimality for feasibility."
      ],
      "metadata": {
        "id": "rw3cH8PfJR6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiOutputLSTM:\n",
        "    def __init__(self, tickers, window_size=30, train_split=0.7, val_split=0.3, units_lstm=150, n_dropout=0.1, hp_lr=1e-3, epochs=20):\n",
        "        # Here’s where we set up the basics: tickers are the stocks we’re tracking, window_size is how much past data we look at.\n",
        "        self.tickers = tickers\n",
        "        self.window_size = window_size\n",
        "        self.train_split = train_split  # Splitting data: 70% for training by default.\n",
        "        self.val_split = val_split      # 30% for validation.\n",
        "        self.units_lstm = units_lstm    # Number of LSTM units—think of it as the brain size of our model.\n",
        "        self.n_dropout = n_dropout      # Dropout rate to keep the model from overfitting.\n",
        "        self.hp_lr = hp_lr              # Learning rate—how fast the model learns.\n",
        "        self.scaler_input = StandardScaler()  # Scales our input data to keep things balanced.\n",
        "        self.scalers_output = {ticker: MinMaxScaler() for ticker in tickers}  # One scaler per ticker for output.\n",
        "        self.model = None               # Placeholder for our LSTM model—we’ll build it later.\n",
        "        self.es = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10, restore_best_weights=True)  # Stops training if we’re stuck.\n",
        "        self.lr_scheduler = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6)  # Tweaks learning rate if needed.\n",
        "        self.n_epochs = epochs          # How many times we’ll train the model. Keep in mind we had to keep this low due to tecnology and time constrains\n",
        "        # These will hold our training, validation, and test data once we prepare it.\n",
        "        self.X_train = None\n",
        "        self.y_train = None\n",
        "        self.train_time = None\n",
        "        self.X_val = None\n",
        "        self.y_val = None\n",
        "        self.val_time = None\n",
        "        self.X_test = None\n",
        "        self.y_test = None\n",
        "        self.test_time = None\n",
        "        self.y_test_original_lagged = None  # Keeps the original test returns for comparison.\n",
        "\n",
        "    def gen_predictors(self, df_ret):\n",
        "        # This method creates features (predictors) from our stock returns data.\n",
        "        df = df_ret.copy()  # Don’t mess with the original data!\n",
        "        for ticker in self.tickers:\n",
        "            # For each stock, we’re making some cool indicators:\n",
        "            df[f\"{ticker}_MA_10\"] = df[ticker].rolling(10).mean()  # 10-day moving average.\n",
        "            df[f\"{ticker}_MA_50\"] = df[ticker].rolling(50).mean()  # 50-day moving average.\n",
        "            df[f\"{ticker}_Volatility_20\"] = df[ticker].rolling(20).std()  # 20-day volatility.\n",
        "            df[f\"{ticker}_Ret25\"] = df[ticker].rolling(25).apply(lambda x: np.prod(1 + x) - 1).shift(-25)  # 25-day future return.\n",
        "        df = df.dropna()  # Drop rows with missing values—clean slate!\n",
        "        # Grab our predictors (X) and target (y) data.\n",
        "        X = df[[f\"{ticker}_MA_10\" for ticker in self.tickers] +\n",
        "               [f\"{ticker}_MA_50\" for ticker in self.tickers] +\n",
        "               [f\"{ticker}_Volatility_20\" for ticker in self.tickers]].values\n",
        "        y = df[[f\"{ticker}_Ret25\" for ticker in self.tickers]].values\n",
        "        return X, y, df.index  # Return predictors, targets, and dates.\n",
        "\n",
        "    def prepare_data(self, X, y, dates):\n",
        "        # Time to split our data into training, validation, and test sets based on dates.\n",
        "        pre_2018_mask = dates <= pd.Timestamp(\"2017-12-31\")  # Everything before 2018 for training/validation.\n",
        "        test_mask = (dates >= pd.Timestamp(\"2018-01-01\")) & (dates <= pd.Timestamp(\"2022-12-30\"))  # 2018-2022 for testing.\n",
        "        pre_2018_X, pre_2018_y, pre_2018_dates = X[pre_2018_mask], y[pre_2018_mask], dates[pre_2018_mask]\n",
        "        test_X, test_y, test_dates = X[test_mask], y[test_mask], dates[test_mask]\n",
        "\n",
        "        # Split pre-2018 data into training and validation.\n",
        "        train_size = int(len(pre_2018_y) * self.train_split)\n",
        "        X_train_set = pre_2018_X[:train_size]\n",
        "        X_val_set = pre_2018_X[train_size:]\n",
        "        y_train_set = pre_2018_y[:train_size]\n",
        "        y_val_set = pre_2018_y[train_size:]\n",
        "        X_test_set, y_test_set = test_X, test_y\n",
        "        self.val_time = pre_2018_dates[train_size:]  # Save validation dates.\n",
        "        self.test_time = test_dates  # Save test dates.\n",
        "\n",
        "        # Scale the data so everything’s on the same playing field.\n",
        "        X_train_scaled = self.scaler_input.fit_transform(X_train_set)\n",
        "        X_val_scaled = self.scaler_input.transform(X_val_set)\n",
        "        X_test_scaled = self.scaler_input.transform(X_test_set)\n",
        "\n",
        "        # Scale the outputs (returns) for each ticker separately.\n",
        "        y_train_scaled = np.column_stack([self.scalers_output[ticker].fit_transform(y_train_set[:, i].reshape(-1, 1))\n",
        "                                         for i, ticker in enumerate(self.tickers)])\n",
        "        y_val_scaled = np.column_stack([self.scalers_output[ticker].transform(y_val_set[:, i].reshape(-1, 1))\n",
        "                                       for i, ticker in enumerate(self.tickers)])\n",
        "        y_test_scaled = np.column_stack([self.scalers_output[ticker].transform(y_test_set[:, i].reshape(-1, 1))\n",
        "                                        for i, ticker in enumerate(self.tickers)])\n",
        "\n",
        "        self.y_test_original_lagged = y_test_set  # Keep the unscaled test returns handy.\n",
        "        return X_train_scaled, X_val_scaled, X_test_scaled, y_train_scaled, y_val_scaled, y_test_scaled\n",
        "\n",
        "    def create_lags(self, X_scaled, y_scaled, dates):\n",
        "        # This creates \"lagged\" sequences—think of it as sliding windows of past data.\n",
        "        X_lagged, y_lagged, time_lagged = [], [], []\n",
        "        for i in range(self.window_size, len(y_scaled)):\n",
        "            X_lagged.append(X_scaled[i - self.window_size:i])  # Grab the past window_size days.\n",
        "            y_lagged.append(y_scaled[i])  # The target for that window.\n",
        "            time_lagged.append(dates[i])  # The corresponding date.\n",
        "        return np.array(X_lagged), np.array(y_lagged), np.array(time_lagged)\n",
        "\n",
        "    def build_model(self, hp, input_shape):\n",
        "        # Here’s where we define our LSTM model architecture with some tunable hyperparameters.\n",
        "        units_lstm = hp.Int('units_lstm', min_value=50, max_value=300, step=50)  # How many LSTM units?\n",
        "        n_dropout = hp.Float('n_dropout', min_value=0.0, max_value=0.5, step=0.1)  # Dropout rate?\n",
        "        hp_lr = hp.Float('hp_lr', min_value=1e-5, max_value=1e-2, sampling='log')  # Learning rate?\n",
        "\n",
        "        model = Sequential([\n",
        "            Input(shape=input_shape),  # Define how big our input is.\n",
        "            LSTM(units_lstm, return_sequences=True),  # First LSTM layer—keeps sequences.\n",
        "            Dropout(n_dropout),  # Prevent overfitting.\n",
        "            LSTM(units_lstm, return_sequences=True),  # Second LSTM layer.\n",
        "            Dropout(n_dropout),\n",
        "            LSTM(units_lstm),  # Final LSTM layer—no sequences this time.\n",
        "            Dropout(n_dropout),\n",
        "            Dense(50, activation=\"relu\"),  # Fully connected layer for some extra processing.\n",
        "            Dense(20, activation=\"relu\"),  # Another one.\n",
        "            Dense(len(self.tickers))  # Output layer—one for each ticker.\n",
        "        ])\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_lr), loss=\"mse\")  # Compile with Adam optimizer.\n",
        "        return model\n",
        "\n",
        "    def tune_hyperparameters(self, X_train, y_train, X_val, y_val, max_trials=5):\n",
        "        # Let’s find the best hyperparameters using Bayesian Optimization!\n",
        "        tuner = kt.BayesianOptimization(\n",
        "            lambda hp: self.build_model(hp, X_train.shape[1:]),\n",
        "            objective='val_loss',  # Minimize validation loss.\n",
        "            max_trials=max_trials,  # How many combos to try.\n",
        "            executions_per_trial=1,\n",
        "            directory='my_dir',\n",
        "            project_name='lstm_tuning',\n",
        "            seed=SEED\n",
        "        )\n",
        "        tuner.search(\n",
        "            X_train, y_train,\n",
        "            epochs=20,\n",
        "            validation_data=(X_val, y_val),\n",
        "            callbacks=[self.es]  # Early stopping to save time.\n",
        "        )\n",
        "        best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]  # Grab the winner.\n",
        "        return best_hps\n",
        "\n",
        "    def prepare_data_for_training(self, df_ret):\n",
        "        # This ties everything together to get our data ready for training.\n",
        "        X, y, dates = self.gen_predictors(df_ret)  # Generate predictors.\n",
        "        X_train_scaled, X_val_scaled, X_test_scaled, y_train_scaled, y_val_scaled, y_test_scaled = self.prepare_data(X, y, dates)\n",
        "        # Create lagged datasets for training, validation, and testing.\n",
        "        self.X_train, self.y_train, self.train_time = self.create_lags(X_train_scaled, y_train_scaled, dates[dates <= pd.Timestamp(\"2017-12-31\")][:len(y_train_scaled)])\n",
        "        self.X_val, self.y_val, self.val_time = self.create_lags(X_val_scaled, y_val_scaled, dates[dates <= pd.Timestamp(\"2017-12-31\")][len(y_train_scaled):])\n",
        "        self.X_test, self.y_test, self.test_time = self.create_lags(X_test_scaled, y_test_scaled, dates[dates >= pd.Timestamp(\"2018-01-01\")])\n",
        "\n",
        "        # Make sure test_time matches the number of lagged samples.\n",
        "        self.test_time = self.test_time[:len(self.y_test)]\n",
        "\n",
        "        # Just checking our shapes to make sure everything lines up.\n",
        "        print(f\"X_test shape: {self.X_test.shape}\")\n",
        "        print(f\"y_test shape: {self.y_test.shape}\")\n",
        "        print(f\"test_time length: {len(self.test_time)}\")\n",
        "        print(f\"y_test_original_lagged shape: {self.y_test_original_lagged.shape}\")\n",
        "\n",
        "    def train(self, tune=False, max_trials=50):\n",
        "        # Time to train the model!\n",
        "        if self.X_train is None or self.y_train is None:\n",
        "            raise ValueError(\"Training data not prepared. Call prepare_data_for_training() first.\")  # Safety check.\n",
        "\n",
        "        if tune:\n",
        "            # If we’re tuning, let’s find the best settings first.\n",
        "            print(\"Starting hyperparameter tuning with Bayesian Optimization...\")\n",
        "            best_hps = self.tune_hyperparameters(self.X_train, self.y_train, self.X_val, self.y_val, max_trials)\n",
        "            self.units_lstm = best_hps.get('units_lstm')\n",
        "            self.n_dropout = best_hps.get('n_dropout')\n",
        "            self.hp_lr = best_hps.get('hp_lr')\n",
        "            print(f\"Best hyperparameters found: units_lstm={self.units_lstm}, n_dropout={self.n_dropout}, hp_lr={self.hp_lr}\")\n",
        "\n",
        "        # Build the LSTM model with our chosen (or default) settings.\n",
        "        self.model = Sequential([\n",
        "            tf.keras.layers.Input(shape=(self.X_train.shape[1], self.X_train.shape[2])),\n",
        "            LSTM(self.units_lstm, return_sequences=True),\n",
        "            Dropout(self.n_dropout),\n",
        "            LSTM(self.units_lstm, return_sequences=True),\n",
        "            Dropout(self.n_dropout),\n",
        "            LSTM(self.units_lstm),\n",
        "            Dropout(self.n_dropout),\n",
        "            Dense(50, activation=\"relu\"),\n",
        "            Dense(20, activation=\"relu\"),\n",
        "            Dense(len(self.tickers))\n",
        "        ])\n",
        "        self.model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.hp_lr), loss=\"mse\")\n",
        "        # Train it and keep track of the loss.\n",
        "        history = self.model.fit(self.X_train, self.y_train, validation_data=(self.X_val, self.y_val),\n",
        "                                epochs=self.n_epochs, callbacks=[self.es, self.lr_scheduler])\n",
        "\n",
        "        self.model.save_weights('model_weights.weights.h5')  # Save the trained weights.\n",
        "\n",
        "        # Plot how the loss changed over time—cool to see!\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        sns.lineplot(x=range(len(history.history['loss'])), y=history.history['loss'], label='Train Loss')\n",
        "        sns.lineplot(x=range(len(history.history['val_loss'])), y=history.history['val_loss'], label='Validation Loss')\n",
        "        plt.legend()\n",
        "        plt.title(\"Training and Validation Loss\")\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.show()\n",
        "        return history\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Make predictions with our trained model.\n",
        "        if self.model is None:\n",
        "            if os.path.exists('model_weights.weights.h5'):\n",
        "                # If the model isn’t loaded but weights exist, rebuild and load them.\n",
        "                self.model = Sequential([\n",
        "                    tf.keras.layers.Input(shape=(X.shape[1], X.shape[2])),\n",
        "                    LSTM(self.units_lstm, return_sequences=True),\n",
        "                    Dropout(self.n_dropout),\n",
        "                    LSTM(self.units_lstm, return_sequences=True),\n",
        "                    Dropout(self.n_dropout),\n",
        "                    LSTM(self.units_lstm),\n",
        "                    Dropout(self.n_dropout),\n",
        "                    Dense(50, activation=\"relu\"),\n",
        "                    Dense(20, activation=\"relu\"),\n",
        "                    Dense(len(self.tickers))\n",
        "                ])\n",
        "                self.model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.hp_lr), loss=\"mse\")\n",
        "                self.model.load_weights('model_weights.weights.h5')\n",
        "            else:\n",
        "                raise ValueError(\"Model not trained. Call train() first.\")  # No model, no predictions!\n",
        "        pred_scaled = self.model.predict(X)  # Get scaled predictions.\n",
        "        # Unscale them back to real-world values.\n",
        "        pred = np.column_stack([self.scalers_output[ticker].inverse_transform(pred_scaled[:, i].reshape(-1, 1))\n",
        "                              for i, ticker in enumerate(self.tickers)])\n",
        "        return pred\n",
        "\n",
        "    def analyze_performance(self):\n",
        "        # Let’s see how well our model did!\n",
        "        if any(var is None or (isinstance(var, np.ndarray) and var.size == 0) for var in [self.X_test, self.y_test, self.y_train]):\n",
        "            raise ValueError(\"Test data not prepared. Ensure prepare_data_for_training() was called.\")  # Safety first.\n",
        "\n",
        "        pred_val = self.predict(self.X_val)  # Predictions for validation.\n",
        "        pred_test = self.predict(self.X_test)  # Predictions for test.\n",
        "\n",
        "        predictions_df = pd.DataFrame(pred_test, index=self.test_time, columns=self.tickers)  # Organize test predictions.\n",
        "        actual_returns_df = pd.DataFrame(self.y_test, index=self.test_time, columns=self.tickers)  # Organize actual test returns.\n",
        "\n",
        "        # Validation plot—comparing actual vs predicted returns over time.\n",
        "        val_df = pd.DataFrame({\n",
        "            'Date': np.tile(self.val_time, len(self.tickers)),\n",
        "            'Actual': np.concatenate([self.scalers_output[ticker].inverse_transform(self.y_val[:, i].reshape(-1, 1)).flatten()\n",
        "                                     for i, ticker in enumerate(self.tickers)]),\n",
        "            'Predicted': np.concatenate([pred_val[:, i] for i in range(len(self.tickers))]),\n",
        "            'Ticker': np.repeat(self.tickers, len(self.val_time))\n",
        "        })\n",
        "        g = sns.FacetGrid(val_df, col='Ticker', col_wrap=1, height=4, aspect=2, sharey=False)\n",
        "        g.map(sns.lineplot, 'Date', 'Actual', label='Actual', color='blue', alpha=0.6)\n",
        "        g.map(sns.lineplot, 'Date', 'Predicted', label='Predicted', color='orange', linestyle='--')\n",
        "        g.add_legend()\n",
        "        g.set_titles(\"{col_name}\")\n",
        "        g.fig.suptitle('Validation Predictions vs Actual Returns', y=1.02)\n",
        "        for ax in g.axes.flat:\n",
        "            ax.set_xlabel('Year')  # Now showing years!\n",
        "            ax.set_ylabel('Return')\n",
        "            ax.tick_params(axis='x', rotation=45)  # Rotate dates for readability.\n",
        "        plt.subplots_adjust(hspace=0.5)\n",
        "        plt.show()\n",
        "\n",
        "        # Test plot—same deal, but for the test period.\n",
        "        test_df = pd.DataFrame({\n",
        "            'Date': np.tile(self.test_time, len(self.tickers)),\n",
        "            'Actual': np.concatenate([self.scalers_output[ticker].inverse_transform(self.y_test[:, i].reshape(-1, 1)).flatten()\n",
        "                                     for i, ticker in enumerate(self.tickers)]),\n",
        "            'Predicted': np.concatenate([pred_test[:, i] for i in range(len(self.tickers))]),\n",
        "            'Ticker': np.repeat(self.tickers, len(self.test_time))\n",
        "        })\n",
        "        g = sns.FacetGrid(test_df, col='Ticker', col_wrap=1, height=4, aspect=2, sharey=False)\n",
        "        g.map(sns.lineplot, 'Date', 'Actual', label='Actual', color='blue', alpha=0.6)\n",
        "        g.map(sns.lineplot, 'Date', 'Predicted', label='Predicted', color='orange', linestyle='--')\n",
        "        g.add_legend()\n",
        "        g.set_titles(\"{col_name}\")\n",
        "        g.fig.suptitle('Test Predictions vs Actual Returns', y=1.02)\n",
        "        for ax in g.axes.flat:\n",
        "            ax.set_xlabel('Year')  # Years here too!\n",
        "            ax.set_ylabel('Return')\n",
        "            ax.tick_params(axis='x', rotation=45)\n",
        "        plt.subplots_adjust(hspace=0.5)\n",
        "        plt.show()\n",
        "\n",
        "        return predictions_df, actual_returns_df  # Return the dataframes for further analysis if needed.\n"
      ],
      "metadata": {
        "id": "2CuA68W5JVvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model = MultiOutputLSTM(tickers)\n",
        "lstm_model.prepare_data_for_training(df_ret)\n",
        "history = lstm_model.train(tune=True, max_trials=5)\n",
        "print(\"Model training completed.\")\n",
        "\n",
        "predictions_df, actual_returns_df = lstm_model.analyze_performance()\n",
        "print(\"Performance analysis completed.\")"
      ],
      "metadata": {
        "id": "lkVUz8pdJXv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performance analysis completed.\n",
        "Best hyperparameters found:\n",
        "\n",
        "units_lstm=300,\n",
        "n_dropout=0.0,\n",
        "hp_lr=0.007902373711581125\n",
        "Loss: Training Loss: 0.0096 Val_loss: 0.0132\n",
        "\n",
        "The neural network was able to find acceptible parameters for training and validation but it fail to generalize it for out of sample. The plots suggest that it was not capable of learning the patterns, it may be a case of overfitting or lack of complexity, also the use of low values for epochs and max trials in the tuner may have limited the results."
      ],
      "metadata": {
        "id": "2B7VMOTsJbet"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.c and d) Trading strategy for out-of-sample and Backtesting:\n",
        "\n",
        "The TradingStrategy class is a Python implementation designed to backtest a predictive trading strategy against a passive buy-and-hold benchmark,\n",
        "\n",
        "Backtest:\n",
        "\n",
        "This method exemplifies a basic backtesting framework, integrating predictive signals with portfolio optimization.\n",
        "\n",
        "Trading Strategy\n",
        "\n",
        "The trading strategy mimics a long-short equity hedge fund approach, leveraging predictions to exploit relative performance. For simplicity some values were disconsidered (e.g., fixed weights, no transaction costs, no leverage constraints)\n",
        "\n",
        "Every few days (based on a set frequency), the strategy looks at predictions for how assets will perform. It picks the top 2 assets expected to do well and invests half the portfolio in each (these are \"long\" positions). Then, it picks the bottom 2 assets expected to do poorly and bets against them by assigning each a negative weight (these are \"short\" positions). All other assets stay at zero.\n",
        "\n",
        "Buy-and-Hold Strategy\n",
        "\n",
        "Setup: Takes the same starting money and splits it equally across all assets—like buying a little bit of everything and holding onto it.\n",
        "\n",
        "Growth Over Time: Watches how each asset performs day by day, letting the initial investment grow or shrink naturally based on those daily changes.\n",
        "\n",
        "Portfolio Value: Adds up the value of all assets each day to see how the total portfolio is doing."
      ],
      "metadata": {
        "id": "Zt5ulZcbJcGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TradingStrategy:\n",
        "    def __init__(self, predictions_df, daily_returns_df, assets, initial_investment=10000, rebalance_freq=5):\n",
        "      # Initialize instance variables with input data and parameters\n",
        "        self.predictions_df = predictions_df\n",
        "        self.daily_returns = daily_returns_df\n",
        "        self.assets = assets\n",
        "        self.initial_investment = initial_investment\n",
        "        self.rebalance_freq = rebalance_freq\n",
        "\n",
        "        # Convert returns to decimal if in percentage form\n",
        "        if self.daily_returns.max().max() > 1:\n",
        "            self.daily_returns = self.daily_returns / 100\n",
        "\n",
        "    def backtest(self):\n",
        "      # Align dates between predictions and returns DataFrames\n",
        "        dates = self.predictions_df.index.intersection(self.daily_returns.index)\n",
        "        preds = self.predictions_df.loc[dates]\n",
        "        rets = self.daily_returns.loc[dates].fillna(0)  # Handle NaNs\n",
        "\n",
        "\n",
        "        # Trading strategy: long top 2 and short bottom 2 assets based on predictions\n",
        "        strategy_value = self.initial_investment\n",
        "        weights = {asset: 0 for asset in self.assets}\n",
        "        strategy_values = []\n",
        "\n",
        "        # Iterate over dates to get strategy returns\n",
        "        for i, date in enumerate(dates):\n",
        "            if i % self.rebalance_freq == 0:\n",
        "                predictions = preds.loc[date]\n",
        "                ranked = predictions.sort_values(ascending=False).index\n",
        "                long_assets = ranked[:2]\n",
        "                short_assets = ranked[-2:]\n",
        "                weights = {asset: 0.5 if asset in long_assets else -0.5 if asset in short_assets else 0\n",
        "                          for asset in self.assets}\n",
        "            # Calculate daily strategy return and update portfolio value\n",
        "            daily_ret_strategy = sum(weights[asset] * rets.loc[date, asset] for asset in self.assets)\n",
        "            strategy_value *= (1 + daily_ret_strategy)\n",
        "            strategy_value = max(strategy_value, 0)\n",
        "            strategy_values.append(strategy_value)\n",
        "\n",
        "\n",
        "        # Buy-and-hold strategy: equal investment in all assets\n",
        "        initial_per_asset = self.initial_investment / len(self.assets) # Initial investment per asset\n",
        "        cum_returns = (1 + rets / 100).cumprod(axis=0) - 1  # Cumulative growth factor (returns are in decimal form)\n",
        "        asset_values = (1 + cum_returns) * initial_per_asset  # Value of each asset over time\n",
        "        bh_daily_values = asset_values.sum(axis=1)  # Total portfolio value\n",
        "\n",
        "        # Create results DataFrame\n",
        "        results = pd.DataFrame({\n",
        "            'Strategy': strategy_values,\n",
        "            'Buy-and-Hold': bh_daily_values\n",
        "        }, index=dates)\n",
        "\n",
        "        # Calculate and print cumulative returns\n",
        "        strat_cum_return = (results['Strategy'].iloc[-1] / self.initial_investment - 1) * 100\n",
        "        bh_cum_return = (results['Buy-and-Hold'].iloc[-1] / self.initial_investment - 1) * 100\n",
        "        print(f\"Cumulative Return - Trading Strategy: {strat_cum_return:.2f}%\")\n",
        "        print(f\"Cumulative Return - Buy-and-Hold: {bh_cum_return:.2f}%\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def plot(self):\n",
        "        \"\"\"Plot the backtest results.\"\"\"\n",
        "\n",
        "        results = self.backtest()\n",
        "        plot_df = results.reset_index().melt(id_vars=['index'], var_name='Strategy', value_name='Value')\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        sns.lineplot(data=plot_df, x='index', y='Value', hue='Strategy')\n",
        "        plt.title('Trading Strategy vs Buy-and-Hold')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Portfolio Value')\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "LZWCgySIJf0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "strategy = TradingStrategy(predictions_df, actual_returns_df, tickers)\n",
        "strategy.plot()"
      ],
      "metadata": {
        "id": "W65h29avJhjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Strategy did not perform well. This may be for the fact that all the assets show some postive returns over time as it can be seen in the buy and hold strategy so shorting assets with positive returns (even limited ones) may have result in the negative trend.\n",
        "\n",
        "Another point is that the neural network did not perform well in testing data so it is unlikely that the predictions reflect the best possible strategies.\n",
        "\n",
        "The long-short strategy from step 2 performed better."
      ],
      "metadata": {
        "id": "oIq-C4fIJl78"
      }
    }
  ]
}